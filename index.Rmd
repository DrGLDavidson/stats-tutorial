---
title: "Statistics Tutorial"
author: "Gabrielle Davidson, School of Biological Sciences, University of East Anglia"
date: "`r Sys.Date()`"
output: 
  html_document:
    mathjax: "default"
    toc: true
    number_sections: true
---

# Getting started

Welcome to the statistics tutorial for datasets typically encountered in the Davidson Lab, School of Biological Sciences, University of East Anglia. This guide will walk you through selecting the appropriate statistical model for your data, formulating the model syntax and variables, testing model assumptions, improving model fit, and performing “model selection.” We will also cover commonly used graphical representations for different types of data.

## Install and load R packages dependencies 
```{r eval=FALSE}
# Install the required packages (if not already installed)
required_packages <- c("tidyverse", "lme4", "MASS", "nlme", "lmerTest", "MuMIn", "ggpubr")

# Install packages that aren't already installed
install_if_needed <- required_packages[!(required_packages %in% installed.packages()[,"Package"])]
if(length(install_if_needed)) install.packages(install_if_needed)

You can also install each manually using the function install.packages("nameOfPackage")

# Load the packages after installation
library(tidyverse)   # A collection of R packages for data science
library(lme4)        # Provides tools for fitting linear and generalized linear mixed-effects models (GLMMs).
library(MASS)        # A large collection of statistical functions, with a focus on generalized linear models (GLMs).
library(nlme)        # Mixed-effects models, and can fit both linear and nonlinear mixed-effects models (U-shaped, sigmoidal, exponential). Model diagnostic tools. 
library(lmerTest)    # Adds p-values to lme4 models
library(MuMIn)       # Model selection & averaging
library(ggplot2)     # Data visualization
library(ggpubr)      # Enhanced ggplot2 functions
```

## Troubleshooting install issues
If you have trouble installing packages try the following: 
* Read the content of the error warning
* Does the package you are installing have dependencies? the error may say "x package not found". Install that package using ```install.packages("nameofpackage")```
* Search the error online
* Search alternative ways to install the package. Sometimes vignettes for the package have direct links you can use for installation, rather than using ```install.packages()```

# Choosing an appropriate statistical test

## Regression models

LMs (Linear Models), GLMs (Generalized Linear Models), and GLMMs (Generalized Linear Mixed Models) are all part of a broad collection of regression models. Each of these models is used to examine relationships between dependent and independent variables, but **they differ in their assumptions, data requirements, and flexibility**.

### Linear Models (LMs):
LMs are the simplest type of regression models. They model the relationship between a dependent variable (response) and one or more independent variables (predictors) using a linear equation.

The model can be written as: 
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \epsilon
$$

Where:
- \( Y \) is the dependent variable, and specifically a continuous variable (i.e. gaussian distribution).
- \( \beta_0 \) is the intercept.
- \( \beta_1, \beta_2, \dots \) are the coefficients for the independent variables \( X_1, X_2, \dots \).
- \( \epsilon \) is the error term.

### Linear Mixed Models (LMMs):

The Linear Mixed Model (LMM) is an extension of the linear model that includes **random effects** to account for correlations within grouped or hierarchical data. These might include repeated measures (i.e. data points collected) from the same individuals, or repeated measures (i.e. data points collected) from the same geographical site. 

The model can be written as:

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + b_0 + \epsilon
$$

Where:
- \( Y \) is the dependent variable, and specifically a continuous variable (i.e. gaussian distribution).
- \( \beta_0 \) is the **fixed intercept**.
- \( \beta_1, \beta_2, \dots \) are the **fixed effect coefficients** for the independent variables \( X_1, X_2, \dots \).
- \( b_0 \) is the **random intercept** associated with each group, representing the deviation of each group’s intercept from the overall intercept. It is assumed to follow a normal distribution with mean 0 and variance \( \sigma^2_b \), i.e., \( b_0 \sim \mathcal{N}(0, \sigma^2_b) \).
- \( \epsilon \sim \mathcal{N}(0, \sigma^2) \) is the **residual error term**, assumed to be normally distributed with mean 0 and variance \( \sigma^2 \).

**What does it mean?** 
- If modeling data with repeated measurements from subjects, \( b_0 \) accounts for individual-specific variations in the intercept.
- The fixed effects (\( \beta_0, \beta_1, \dots \)) represent the overall average relationship between predictors and the outcome, while the random effects capture variation at the group level.

This formulation allows the model to handle **correlated data** within groups, such as multiple measurements from the same subject or repeated observations from different locations.

### Generalised Linear Models (GLMs):

GLMs extend linear models by allowing for response variables that have distributions other than the normal distribution (e.g., binomial, Poisson).

They introduce the concept of a link function to model the relationship between the predictors and the mean of the response variable.

GLMs are used when the response variable is categorical (e.g., logistic regression for binary outcomes) or count data (e.g., Poisson regression).

The model can be written as: 
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \epsilon
$$

Where:
- \( Y \) is the dependent variable which can follow distributions such as **binary** (e.g., for logistic regression) or **count** (e.g., for Poisson regression).
- \( \beta_0 \) is the intercept.
- \( \beta_1, \beta_2, \dots \) are the coefficients for the independent variables \( X_1, X_2, \dots \).
- \( \epsilon \) is the error term.

### Generalised Linear Mixed Models (GLMMs)

The Generalized Linear Mixed Model (GLMM) extends the Generalized Linear Model (GLM) by incorporating **random effects**, which allow for hierarchical or grouped data structures.

The model can be written as:

$$
Y = g^{-1} \left( \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + b_0 + \epsilon \right)
$$

Where:
- \( Y \) is the dependent variable which can follow distributions such as **binary** (e.g., for logistic regression) or **count** (e.g., for Poisson regression).
- \( g^{-1} \) is the **inverse link function**, mapping the linear predictor to the expected value of \( Y \).
- \( \beta_0 \) is the **fixed intercept**.
- \( \beta_1, \beta_2, \dots \) are the **fixed effect coefficients** for the independent variables \( X_1, X_2, \dots \).
- \( b_0 \sim \mathcal{N}(0, \sigma^2_b) \) is the **random intercept**, assumed to follow a normal distribution with mean 0 and variance \( \sigma^2_b \).
- \( \epsilon \sim \mathcal{N}(0, \sigma^2) \) is the **residual error term**.

**What does it mean?**
- If modeling repeated measures, \( b_0 \) represents variation in the intercept for different individuals.
- If modeling data from multiple locations, \( b_0 \) accounts for location-specific differences.

This formulation allows the model to **capture dependencies** within grouped data while retaining the flexibility of a generalized linear framework.

### Selecting the appropriate regression model for your data

Simply put, follow the workflow (Figure 1) to select the right regression model for your data.


```{r out.width="60%", out.height="60%", echo=FALSE, fig.align="center"}
knitr::include_graphics("F:/RWorkspace/GitHub/stats-tutorial/data/regression_workflow.tif")

```
<center>
Figure 1. Regression model selection workflow
</center>



## Principal Component Analysis (PCA)

Principal Component Analysis (PCA) is a **dimensionality reduction** technique used to simplify large datasets by transforming the data into a smaller set of uncorrelated variables called **principal components**. These components explain the variance in the original data, with the first principal component explaining the greatest variance.

For example, imagine you have a dataset containing scores from an intelligence test, where multiple measures (such as memory, reasoning, problem-solving, etc.) are recorded. These measures may be correlated, as they all assess aspects of cognitive ability. PCA can be used to combine these correlated variables into a smaller number of principal components, capturing the main variation in the data without losing significant information.

The PCA model can be written as:

$$
\mathbf{X} = \mathbf{T} \mathbf{P}^T + \mathbf{E}
$$

Where:
- \( \mathbf{X} \) is the original data matrix (with variables as columns and observations as rows).
- \( \mathbf{T} \) is the **scores matrix**, where each row represents the transformed data in terms of the principal components.
- \( \mathbf{P} \) is the **loading matrix**, representing the eigenvectors (directions) of the original data that correspond to the principal components.
- \( \mathbf{E} \) is the **residual matrix**, representing the part of the original data that cannot be explained by the principal components.

**What does it mean?**
- PCA identifies the axes (principal components) that maximize the variance in the data.
- The first principal component \( \mathbf{t}_1 \) explains the greatest variance, while subsequent components \( \mathbf{t}_2, \mathbf{t}_3, \dots \) explain progressively less variance.
- The loadings matrix \( \mathbf{P} \) shows how each original variable contributes to the principal components.

PCA is commonly used for:
- **Data reduction**, where a smaller number of principal components are used to represent the data with minimal loss of information.
- **Visualisation** of high-dimensional data, typically in 2D or 3D plots, by using the first two or three principal components.


# Regression models in R

## Basic model code syntax

The R package, the function and its syntax depends on the type of model you are running. 

**LM syntax**

```model <- lm(response ~ predictor1 + predictor2, data = your_data)```

**LMM syntax**

```model <- lmer(response ~ predictor1 + (1|group), data = your_data, family = gaussian)```

**GLM syntax**

```model <- glm(response ~ predictor1 + predictor2, family = binomial, data = your_data)```

```model <- glm(response ~ predictor1 + predictor2, family = poisson, data = your_data)```

**GLMM syntax**

```model <- glmer(response ~ predictor1 + predictor2 + (1 | group), family = binomial, data = your_data)```

GLMMS can be run using a slightly different package that can handle non-linear structures and provides AIC and BIC output scores:  

```model <- lme(response ~ predictor1 + predictor2, random = ~ 1 | group, data = your_data)```

where ```reponse``` is your dependent variable, ```predictor1``` and ```predictor2``` are your dependent variables, also known as **fixed effects**, and ```group``` is your repeated measures, such as individual or site, also known as **random effects**.

## Including interaction terms in your model code syntax

Interactions in statistical models allow you to explore how the relationship between one predictor and the outcome changes depending on the value of another predictor.

Instead of simply including main fixed effects ```response ~ predictor1 + predictor2``` , to test an interaction between fixed effects, change the syntax by replacing the ```+``` with a ```*```  ```response ~ predictor1*predictor2```

**Example**
Imagine you measured the cognition of birds, but the birds were of different sex and age; including an interaction in your model allows you to explore how the relationship between age and cognition changes depending on the bird's sex.

For example, age may positively affect cognition in males but not in females. older males may have better cognition than younger males. or perhaps older females might have a lower cognition compared to younger females, perhaps due to differences in reproductive energy allocation or other biological factors.

Ignoring the interaction could lead to incorrect inferences. For instance, a model without the interaction term might incorrectly assume that age affects males and females the same way.

That being said, be sure to only include interaction terms if you have an a priori reason to test the interaction, based on existing literature, ecological theory or speculations based on observations. 

## Example dataset and models

### GLM gaussian distribution with random effects: 

```{r}
set.seed(123)  # Ensures the random code is reproducible 

# Number of rows
n <- 100

# Create mock data
bird_data <- data.frame(
  # Randomly assign age (adult or juvenile)
  age = factor(sample(c("adult", "juvenile"), n, replace = TRUE)),
  
  # Randomly assign sex (male or female)
  sex = factor(sample(c("male", "female"), n, replace = TRUE)),
  
  # Randomly assign site (3 different sites)
  site = factor(sample(1:3, n, replace = TRUE)),
  
  # Generate continuous body_size data (e.g., normally distributed)
  body_size = rnorm(n, mean = 50, sd = 10)  # Mean = 50, SD = 10
)

# Now, generate cognition based on the predictors (age, sex, body_size) and their interaction
# We'll add some noise with rnorm() to simulate data
bird_data$cognition <- with(bird_data, 
                            10 + 3*(age == "juvenile") + 2*(sex == "male") + 
                              1.5*(age == "juvenile" & sex == "male") + 0.05*body_size + 
                              rnorm(n, mean = 0, sd = 5))  # Add random noise

```

```{r}
# Check the first few rows of the dataset
head(bird_data)
```

```{r}
# Run the model
library(lme4) # For the lmer() function
library(lmerTest) # To generate p-values

# Fit the linear mixed model with age, sex, body_size, and their interaction
model <- lmer(cognition ~ age * sex + body_size + (1 | site), data = bird_data)
```

Generate the statistical output of your model
```{r}
# Summary of the model
summary(model)
```

### model interpretation

TO COME

### GLMM binomial distribution with random effects: 
```{r}
# Create a binary variable for proportion of correct choices (e.g., cognition > 15 is correct, otherwise incorrect)
bird_data$correct_choice <- ifelse(bird_data$cognition > 15, 1, 0)

head(bird_data)
```

```{r}
# Run the model
# Fit a mixed-effects logistic regression model
model2 <- glmer(correct_choice ~ age * sex + body_size + (1 | site), 
                family = binomial(link = "logit"), 
                data = bird_data)
```

Generate the statistical output of your model
```{r}
# Check the summary of the model
summary(model2)
```

